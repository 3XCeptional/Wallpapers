#!/usr/bin/env python3

import os
import argparse
import shutil
import hashlib
import re
from datetime import datetime
from pathlib import Path
from PIL import Image, UnidentifiedImageError
from typing import List, Dict, Tuple, Optional

# --- Constants ---
IMAGES_PER_PAGE = 30
KEYWORDS = {
    'anime': ['anime', 'manga', 'waifu'],
    'nature': ['nature', 'landscape', 'mountain', 'ocean', 'sky', 'forest'],
    'wallpapers': ['wallpaper', 'wallpapers', 'background'],
    'woman': ['woman', 'girl', 'model'],
    'chick': ['chick', 'babe', 'hot'],
    'general': [] # Fallback category
}

# --- Templates ---

MARKDOWN_TEMPLATE = """
## {image_name} | SIZE: {width}x{height}px | Orientation: {orientation} | [Download]({relative_path})

![{alt_text}]({relative_path} "{title_text}")
"""

README_TEMPLATE = """
# Image Gallery Documentation

This collection was automatically generated by a Python script.

- **Directory Scanned:** `{source_dir}`
- **Execution Timestamp:** `{timestamp}`
- **Total Unique Images Processed:** {image_count}
- **Total Markdown Pages Generated:** {page_count}

---

## Filename Sanitization

All image filenames have been sanitized to ensure URL compatibility for Markdown links. Spaces and non-standard characters have been replaced with underscores (`_`).

---

## Navigation

Below are the links to all the generated gallery pages:

{page_links}

---

## Disclaimer

**All images in this collection are the property of their respective creators or copyright holders.**

I do not claim ownership of any image. This repository serves only as my personal archive and is not intended for commercial distribution. The organization and documentation are for personal reference and convenience.
"""

MIT_LICENSE_TEXT = """
MIT License

Copyright (c) {year} {author}

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
"""

# --- Helper Functions ---

def sanitize_filename(filename: str) -> str:
    """Replaces spaces and non-URL-safe characters with underscores."""
    # Remove file extension for sanitization
    name, ext = os.path.splitext(filename)
    # Replace non-alphanumeric characters (except dots and hyphens) with underscore
    sanitized_name = re.sub(r'[^a-zA-Z0-9.-]', '_', name)
    # Replace multiple underscores with a single one
    sanitized_name = re.sub(r'__+', '_', sanitized_name)
    return f"{sanitized_name}{ext}"

def get_image_hash(filepath: Path) -> str:
    """Generates a SHA256 hash for a file to detect duplicates."""
    h = hashlib.sha256()
    with open(filepath, 'rb') as f:
        while True:
            chunk = f.read(8192)
            if not chunk:
                break
            h.update(chunk)
    return h.hexdigest()

def get_resolution_bucket(width: int, height: int) -> str:
    """Categorizes an image into a resolution bucket."""
    major_dim = max(width, height)
    if major_dim < 1920:
        return 'sub_1080p'
    elif 1920 <= major_dim < 2560:
        return '1080p'
    elif 2560 <= major_dim < 3840:
        return '2K'
    elif 3840 <= major_dim < 7680:
        return '4K'
    else:
        return 'above_4K'

def get_keyword_tag(path_str: str) -> str:
    """Detects a keyword from a path string (filename or folder name)."""
    path_lower = path_str.lower()
    for tag, keys in KEYWORDS.items():
        if any(key in path_lower for key in keys):
            return tag
    return 'general'

# --- Core Functions ---

def process_images(source_dir: Path, output_dir: Path) -> List[Dict]:
    """
    Scans for images, sanitizes, categorizes, copies, and collects metadata.
    """
    print("Starting image discovery and processing...")
    processed_images = []
    seen_hashes = set()
    image_paths = list(source_dir.rglob("*.jpg"))
    total_images = len(image_paths)

    for i, image_path in enumerate(image_paths):
        print(f"  -> Processing [{i+1}/{total_images}]: {image_path.name}")
        
        # 1. Avoid Duplicates
        image_hash = get_image_hash(image_path)
        if image_hash in seen_hashes:
            print(f"     - Skipping duplicate content: {image_path.name}")
            continue
        seen_hashes.add(image_hash)

        # 2. Get Image Info
        try:
            with Image.open(image_path) as img:
                width, height = img.size
        except UnidentifiedImageError:
            print(f"     - Skipping corrupted/unidentified image: {image_path.name}")
            continue

        orientation = "Vertical" if height > width else "Horizontal"
        
        # 3. Sanitize and Categorize
        sanitized_name_str = sanitize_filename(image_path.name)
        resolution_bucket = get_resolution_bucket(width, height)
        # Check both file and parent folder names for keywords
        keyword_tag = get_keyword_tag(str(image_path.parent.name) + sanitized_name_str)

        # 4. Create New Directory Structure and Copy Image
        dest_category_dir = (
            output_dir / 
            orientation.lower() / 
            resolution_bucket / 
            keyword_tag /
            image_path.parent.relative_to(source_dir)
        )
        dest_category_dir.mkdir(parents=True, exist_ok=True)
        
        dest_image_path = dest_category_dir / sanitized_name_str
        shutil.copy2(image_path, dest_image_path)
        
        # 5. Prepare Markdown Data
        relative_image_path = dest_image_path.relative_to(output_dir).as_posix()
        image_name_no_ext = Path(sanitized_name_str).stem
        
        markdown_block = MARKDOWN_TEMPLATE.format(
            image_name=image_name_no_ext.replace('_', ' '),
            width=width,
            height=height,
            orientation=orientation,
            relative_path=relative_image_path,
            alt_text=f"{orientation} image: {image_name_no_ext}",
            title_text=f"{image_name_no_ext} - {width}x{height}"
        )
        
        processed_images.append({'markdown': markdown_block})

    print(f"\nProcessing complete. Found {len(processed_images)} unique images.")
    return processed_images

def generate_markdown_files(output_dir: Path, images_data: List[Dict]) -> List[str]:
    """Generates paginated Markdown files from image data."""
    if not images_data:
        print("No images to process. Skipping Markdown file generation.")
        return []

    print("Generating paginated Markdown files...")
    page_count = (len(images_data) - 1) // IMAGES_PER_PAGE + 1
    page_names = []

    for i in range(page_count):
        page_num = i + 1
        start_index = i * IMAGES_PER_PAGE
        end_index = start_index + IMAGES_PER_PAGE
        page_data = images_data[start_index:end_index]
        
        page_content = "\n".join([item['markdown'] for item in page_data])
        page_filename = f"wallpapers_page_{page_num}.md"
        page_path = output_dir / page_filename
        
        with open(page_path, 'w', encoding='utf-8') as f:
            f.write(page_content)
        
        page_names.append(page_filename)
        print(f"  -> Created {page_filename} with {len(page_data)} entries.")

    return page_names

def generate_readme(output_dir: Path, source_dir: Path, image_count: int, page_names: List[str]):
    """Generates the README.md file."""
    print("Generating README.md...")
    page_links = "\n".join(f"- [Page {i+1}]({name})" for i, name in enumerate(page_names))
    
    readme_content = README_TEMPLATE.format(
        source_dir=source_dir.resolve(),
        timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        image_count=image_count,
        page_count=len(page_names),
        page_links=page_links
    )
    
    with open(output_dir / "README.md", 'w', encoding='utf-8') as f:
        f.write(readme_content)
    print("  -> README.md generated successfully.")

def generate_license(output_dir: Path):
    """Generates the LICENSE file."""
    print("Generating LICENSE file...")
    license_content = MIT_LICENSE_TEXT.format(
        year=datetime.now().year,
        author="The Script Author"
    )
    with open(output_dir / "LICENSE", 'w', encoding='utf-8') as f:
        f.write(license_content)
    print("  -> LICENSE file generated successfully.")

# --- Main Execution ---

def main():
    """Main function to parse arguments and orchestrate the script."""
    parser = argparse.ArgumentParser(
        description="Scans a directory for JPG images, categorizes them, and generates paginated Markdown galleries."
    )
    parser.add_argument(
        "-d", "--directory",
        type=Path,
        required=True,
        help="Root directory to search for images recursively."
    )
    parser.add_argument(
        "-o", "--output",
        type=Path,
        required=True,
        help="Output directory to save Markdown files, images, README, and license."
    )
    
    args = parser.parse_args()

    source_dir = args.directory
    output_dir = args.output

    # --- Validation ---
    if not source_dir.is_dir():
        print(f"Error: Source directory not found at '{source_dir}'")
        return
    
    # --- Setup ---
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # --- Orchestration ---
    processed_data = process_images(source_dir, output_dir)
    page_names = generate_markdown_files(output_dir, processed_data)
    generate_readme(output_dir, source_dir, len(processed_data), page_names)
    generate_license(output_dir)

    print("\n✅ All tasks completed successfully!")

if __name__ == "__main__":
    main()